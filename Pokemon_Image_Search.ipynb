{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pokemon Image Search",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karta2552442a/jacky/blob/master/Pokemon_Image_Search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYU7edJQS160",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "!mkdir dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m41g5H9el2-m",
        "colab_type": "text"
      },
      "source": [
        "用mkdir指令建立圖像資料夾dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8g912_l-bch8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir dataset/Gengar\n",
        "!mkdir dataset/Charizard\n",
        "!mkdir dataset/Dragonite\n",
        "!mkdir dataset/Gyarados\n",
        "!mkdir dataset/Snorlax\n",
        "\n",
        "#dataset/Gengar\n",
        "#dataset/Charizard\n",
        "#dataset/Dragonite\n",
        "#dataset/Gyarados\n",
        "#dataset/Snorlax"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQaVra5imANe",
        "colab_type": "text"
      },
      "source": [
        "再用mkdir指令在建立好的dataset資料夾中新增五種寶可夢的資料夾，用來保存圖片"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcVt18WwT4Rl",
        "colab_type": "code",
        "outputId": "66e91f42-12db-4875-93f7-8eb16fdea18f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "! pip install requests\n",
        "from requests import exceptions\n",
        "import requests\n",
        "import cv2\n",
        "import os\n",
        "import gevent\n",
        "\n",
        "# poke name to download\n",
        "pokemon = 'Gengar'\n",
        "output = 'dataset/Gengar'\n",
        "\n",
        "API_KEY = \"b8c532437a5d42efb755e2e9d840df1e\"\n",
        "MAX_RESULTS = 250\n",
        "GROUP_SIZE = 50\n",
        "\n",
        "# set the endpoint API URL\n",
        "URL = \"https://api.cognitive.microsoft.com/bing/v7.0/images/search\"\n",
        "\n",
        "# when attempting to download images from the web both the Python\n",
        "# programming language and the requests library have a number of\n",
        "# exceptions that can be thrown so let's build a list of them now\n",
        "# so we can filter on them\n",
        "EXCEPTIONS = {IOError, FileNotFoundError, exceptions.RequestException, exceptions.HTTPError, exceptions.ConnectionError,\n",
        "              exceptions.Timeout}\n",
        "\n",
        "# store the search term in a convenience variable then set the\n",
        "# headers and search parameters\n",
        "term = pokemon\n",
        "headers = {\"Ocp-Apim-Subscription-Key\": API_KEY}\n",
        "params = {\"q\": term, \"offset\": 0, \"count\": GROUP_SIZE}\n",
        "\n",
        "# make the search\n",
        "print(\"[INFO] searching Bing API for '{}'\".format(term))\n",
        "search = requests.get(URL, headers=headers, params=params)\n",
        "search.raise_for_status()\n",
        "\n",
        "# grab the results from the search, including the total number of\n",
        "# estimated results returned by the Bing API\n",
        "results = search.json()\n",
        "estNumResults = min(results[\"totalEstimatedMatches\"], MAX_RESULTS)\n",
        "print(\"[INFO] {} total results for '{}'\".format(estNumResults, term))\n",
        "\n",
        "# initialize the total number of images downloaded thus far\n",
        "total = 0\n",
        "\n",
        "\n",
        "def grab_page(url, ext, total):\n",
        "\n",
        "    try:\n",
        "        # total += 1\n",
        "        print(\"[INFO] fetching: {}\".format(url))\n",
        "        r = requests.get(url, timeout=30)\n",
        "        # build the path to the output image\n",
        "\n",
        "        #here total is only for filename creation\n",
        "        p = os.path.sep.join([output, \"{}{}\".format(\n",
        "            str(total).zfill(8), ext)])\n",
        "\n",
        "        # write the image to disk\n",
        "        f = open(p, \"wb\")\n",
        "        f.write(r.content)\n",
        "        f.close()\n",
        "\n",
        "        # try to load the image from disk\n",
        "        image = cv2.imread(p)\n",
        "\n",
        "        # if the image is `None` then we could not properly load the\n",
        "        # image from disk (so it should be ignored)\n",
        "        if image is None:\n",
        "            print(\"[INFO] deleting: {}\".format(p))\n",
        "            os.remove(p)\n",
        "            return\n",
        "\n",
        "    # catch any errors that would not unable us to download the\n",
        "    # image\n",
        "    except Exception as e:\n",
        "        # check to see if our exception is in our list of\n",
        "        # exceptions to check for\n",
        "        if type(e) in EXCEPTIONS:\n",
        "            print(\"[INFO] skipping: {}\".format(url))\n",
        "            return \n",
        "\n",
        "# loop over the estimated number of results in `GROUP_SIZE` groups\n",
        "for offset in range(0, estNumResults, GROUP_SIZE):\n",
        "    # update the search parameters using the current offset, then\n",
        "    # make the request to fetch the results\n",
        "    print(\"[INFO] making request for group {}-{} of {}...\".format(\n",
        "        offset, offset + GROUP_SIZE, estNumResults))\n",
        "    params[\"offset\"] = offset\n",
        "    search = requests.get(URL, headers=headers, params=params)\n",
        "    search.raise_for_status()\n",
        "    results = search.json()\n",
        "    print(\"[INFO] saving images for group {}-{} of {}...\".format(\n",
        "        offset, offset + GROUP_SIZE, estNumResults))\n",
        "    # loop over the results\n",
        "    jobs = []\n",
        "    for v in results[\"value\"]:\n",
        "        total += 1\n",
        "        ext = v[\"contentUrl\"][v[\"contentUrl\"].rfind(\".\"):]\n",
        "        url = v[\"contentUrl\"]\n",
        "        \n",
        "        # create gevent job\n",
        "        jobs.append(gevent.spawn(grab_page, url, ext, total))\n",
        "\n",
        "    # wait for all jobs to complete\n",
        "    gevent.joinall(jobs, timeout=10)\n",
        "    print(total)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (2.21.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests) (2019.3.9)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests) (3.0.4)\n",
            "[INFO] searching Bing API for 'Gengar'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-d10df5d036ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[INFO] searching Bing API for '{}'\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0msearch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mURL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0msearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# grab the results from the search, including the total number of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Access Denied for url: https://api.cognitive.microsoft.com/bing/v7.0/images/search?q=Gengar&offset=0&count=50"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1lShaCLmuwO",
        "colab_type": "text"
      },
      "source": [
        "利用Microsoft’s Bing Image Search的功能搜尋圖片，先在Microsoft的Cognitive Services申請Bing Search專用的API KEY，pokemon用寶可夢的名字作為搜尋的關鍵字，搜尋到的圖片若可以存取便儲存在剛剛建好的資料夾，以output定義存入圖片的路徑，GROUP_SIZE=50代表抓圖時以50張為一組，設定MAX_RESULTS=250，總共250張圖片作為樣本"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1wHkxodqPts",
        "colab_type": "code",
        "outputId": "fd5df056-c55d-4d00-97da-9efbafd2e9a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!git clone https://github.com/jrosebr1/imutils.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'imutils'...\n",
            "remote: Enumerating objects: 660, done.\u001b[K\n",
            "remote: Total 660 (delta 0), reused 0 (delta 0), pack-reused 660\n",
            "Receiving objects: 100% (660/660), 6.88 MiB | 13.46 MiB/s, done.\n",
            "Resolving deltas: 100% (383/383), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbr46QacsWxh",
        "colab_type": "text"
      },
      "source": [
        "clone imutils工具到程式中，imutils為圖像處理的工具"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06IOS5VarZcX",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Thx09btMtoVz",
        "colab_type": "code",
        "outputId": "3e03814d-4d14-463d-aaba-8bf86197ac47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# import the necessary packages\n",
        "from keras.models import Sequential\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.layers.core import Activation\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Dense\n",
        "from keras import backend as K\n",
        "\n",
        "class SmallerVGGNet:\n",
        "\t@staticmethod\n",
        "\tdef build(width, height, depth, classes):\n",
        "\t\t# initialize the model along with the input shape to be\n",
        "\t\t# \"channels last\" and the channels dimension itself\n",
        "\t\tmodel = Sequential()\n",
        "\t\tinputShape = (height, width, depth)\n",
        "\t\tchanDim = -1\n",
        " \n",
        "\t\t# if we are using \"channels first\", update the input shape\n",
        "\t\t# and channels dimension\n",
        "\t\tif K.image_data_format() == \"channels_first\":\n",
        "\t\t\tinputShape = (depth, height, width)\n",
        "\t\t\tchanDim = 1\n",
        "      \n",
        "      # CONV => RELU => POOL\n",
        "\t\tmodel.add(Conv2D(32, (3, 3), padding=\"same\",\n",
        "\t\t\tinput_shape=inputShape))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\tmodel.add(MaxPooling2D(pool_size=(3, 3)))\n",
        "\t\tmodel.add(Dropout(0.25))\n",
        "    \n",
        "    # (CONV => RELU) * 2 => POOL\n",
        "\t\tmodel.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\tmodel.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\t\tmodel.add(Dropout(0.25))\n",
        "    \n",
        "    # (CONV => RELU) * 2 => POOL\n",
        "\t\tmodel.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\tmodel.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\t\tmodel.add(Dropout(0.25))\n",
        "    \n",
        "    # first (and only) set of FC => RELU layers\n",
        "\t\tmodel.add(Flatten())\n",
        "\t\tmodel.add(Dense(1024))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization())\n",
        "\t\tmodel.add(Dropout(0.5))\n",
        " \n",
        "\t\t# softmax classifier\n",
        "\t\tmodel.add(Dense(classes))\n",
        "\t\tmodel.add(Activation(\"softmax\"))\n",
        " \n",
        "\t\t# return the constructed network architecture\n",
        "\t\treturn model\n",
        "  \n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJfBcjOXtANq",
        "colab_type": "text"
      },
      "source": [
        "建立SmallerVGGNet模組，引入keras的模組，定義四個參數width, height, depth, classes，分別是圖像寬度，圖像高度，圖像的深度及數據集中的類數，即寶可夢的種類數，接著加入layer到模組中，首先是CONV => RELU => POOL，卷積層有32個filters和3X3的kernel，POOL層使用 3 x 3大小將空間尺寸從96 x 96 快速縮小到32 x 32，並用dropout25%來隨機斷開目前的layer防止過度擬合，接著是（CONV = > RELU ）* 2，多個CONV和 RELU層堆疊 在一起 ，將filter大小從32增加到64，並把pool size從3X3減少到2X2，接著再加入一組（CONV = > RELU ）* 2 = > POOL，filter大小增加到128，Dense(1024)用來完全連接指定的layer，並把數據線性及歸一化，在fully-connected layers執行最後的dropout，大小為50%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjVnSJB5nG96",
        "colab_type": "code",
        "outputId": "81fb1342-bae7-481c-fdce-2e4306dae8c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "! pip install --upgrade imutils"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: imutils in /usr/local/lib/python3.6/dist-packages (0.5.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe5p6IogtCvi",
        "colab_type": "text"
      },
      "source": [
        "更新imutils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmHBCoarvErL",
        "colab_type": "code",
        "outputId": "69e7eae4-df6f-4d92-8a2b-758ec557d24d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3692
        }
      },
      "source": [
        "# set the matplotlib backend so figures can be saved in the background\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "\n",
        "# import the necessary packages\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "#from smallervggnet import SmallerVGGNet\n",
        "import matplotlib.pyplot as plt\n",
        "from imutils import paths\n",
        "import numpy as np\n",
        "import argparse\n",
        "import random\n",
        "import pickle\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "# construct the argument parse and parse the arguments\n",
        "dataset= 'dataset'\n",
        "modelz='pokedex.model'\n",
        "labelbin='lb.pickle'\n",
        "plot='plot.png'\n",
        "\n",
        "\n",
        "#args = easydict.EasyDict({\n",
        "        #\"dataset\":True,\n",
        "        #\"model\":True,\n",
        "        #\"labelbin\":True,\n",
        "      #  \"plot\":\"plot.png\",\n",
        "        \n",
        "#})\n",
        "\n",
        " #construct the argument parse and parse the arguments\n",
        "\"\"\"ap = argparse.ArgumentParser()\n",
        "ap.add_argument(\"-d\", \"--dataset\", required=True,\n",
        "\thelp=\"path to input dataset (i.e., directory of images)\")\n",
        "ap.add_argument(\"-m\", \"--model\", required=True,\n",
        "  help=\"path to output model\")\n",
        "ap.add_argument(\"-l\", \"--labelbin\", required=True,\n",
        "\thelp=\"path to output label binarizer\")\n",
        "ap.add_argument(\"-p\", \"--plot\", type=str, default=\"plot.png\",\n",
        "\thelp=\"path to output accuracy/loss plot\")\n",
        "args = vars(ap.parse_args())\"\"\"\n",
        "\n",
        "# initialize the number of epochs to train for, initial learning rate,\n",
        "# batch size, and image dimensions\n",
        "EPOCHS = 100\n",
        "INIT_LR = 1e-3\n",
        "BS = 32\n",
        "IMAGE_DIMS = (96, 96, 3)\n",
        "\n",
        "# initialize the data and labels\n",
        "data = []\n",
        "labels = []\n",
        "\n",
        "\n",
        "# grab the image paths and randomly shuffle them\n",
        "print(\"[INFO] loading images...\")\n",
        "#imagePaths = sorted(list(paths.list_images(args[\"dataset\"])))\n",
        "imagePaths = sorted(list(paths.list_images(dataset)))\n",
        "random.seed(42)\n",
        "random.shuffle(imagePaths)\n",
        "\n",
        "# loop over the input images\n",
        "for imagePath in imagePaths:\n",
        "\t# load the image, pre-process it, and store it in the data list\n",
        "\timage = cv2.imread(imagePath)\n",
        "\timage = cv2.resize(image, (IMAGE_DIMS[1], IMAGE_DIMS[0]))\n",
        "\timage = img_to_array(image)\n",
        "\tdata.append(image)\n",
        " \n",
        "\t# extract the class label from the image path and update the\n",
        "\t# labels list\n",
        "\tlabel = imagePath.split(os.path.sep)[-2]\n",
        "\tlabels.append(label)\n",
        "\n",
        "# scale the raw pixel intensities to the range [0, 1]\n",
        "data = np.array(data, dtype=\"float\") / 255.0\n",
        "labels = np.array(labels)\n",
        "print(\"[INFO] data matrix: {:.2f}MB\".format(\n",
        "\tdata.nbytes / (1024 * 1000.0)))\n",
        "\n",
        "# binarize the labels\n",
        "lb = LabelBinarizer()\n",
        "labels = lb.fit_transform(labels)\n",
        "\n",
        "# partition the data into training and testing splits using 80% of\n",
        "# the data for training and the remaining 20% for testing\n",
        "(trainX, testX, trainY, testY) = train_test_split(data,\n",
        "\tlabels, test_size=0.2, random_state=42)\n",
        "\n",
        "# construct the image generator for data augmentation\n",
        "aug = ImageDataGenerator(rotation_range=25, width_shift_range=0.1,\n",
        "\theight_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n",
        "\thorizontal_flip=True, fill_mode=\"nearest\")\n",
        "\n",
        "# initialize the model\n",
        "print(\"[INFO] compiling model...\")\n",
        "model = SmallerVGGNet.build(width=IMAGE_DIMS[1], height=IMAGE_DIMS[0],\n",
        "\tdepth=IMAGE_DIMS[2], classes=len(lb.classes_))\n",
        "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
        "\tmetrics=[\"accuracy\"])\n",
        "\n",
        "# train the network\n",
        "print(\"[INFO] training network...\")\n",
        "H = model.fit_generator(\n",
        "\taug.flow(trainX, trainY, batch_size=BS),\n",
        "\tvalidation_data=(testX, testY),\n",
        "\tsteps_per_epoch=len(trainX) // BS,\n",
        "\tepochs=EPOCHS, verbose=1)\n",
        "\n",
        "# save the model to disk\n",
        "print(\"[INFO] serializing network...\")\n",
        "#model.save(args[\"model\"])\n",
        "model.save(modelz)\n",
        "\n",
        "# save the label binarizer to disk\n",
        "print(\"[INFO] serializing label binarizer...\")\n",
        "#f = open(args[\"labelbin\"], \"wb\")\n",
        "f = open(labelbin, \"wb\")\n",
        "f.write(pickle.dumps(lb))\n",
        "f.close()\n",
        "\n",
        "# plot the training loss and accuracy\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "N = EPOCHS\n",
        "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"acc\"], label=\"train_acc\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_acc\"], label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"upper left\")\n",
        "#plt.savefig(args[\"plot\"])\n",
        "plt.savefig(plot)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading images...\n",
            "[INFO] data matrix: 232.42MB\n",
            "[INFO] compiling model...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "[INFO] training network...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/100\n",
            "26/26 [==============================] - 54s 2s/step - loss: 2.3762 - acc: 0.3906 - val_loss: 2.7150 - val_acc: 0.4444\n",
            "Epoch 2/100\n",
            "26/26 [==============================] - 51s 2s/step - loss: 1.4879 - acc: 0.5244 - val_loss: 2.9183 - val_acc: 0.4213\n",
            "Epoch 3/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 1.3863 - acc: 0.5453 - val_loss: 1.7475 - val_acc: 0.5139\n",
            "Epoch 4/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 1.2099 - acc: 0.5967 - val_loss: 1.4746 - val_acc: 0.5602\n",
            "Epoch 5/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 1.1384 - acc: 0.6116 - val_loss: 2.9033 - val_acc: 0.4583\n",
            "Epoch 6/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 1.3608 - acc: 0.5285 - val_loss: 2.7381 - val_acc: 0.4769\n",
            "Epoch 7/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 1.2304 - acc: 0.6059 - val_loss: 1.7354 - val_acc: 0.6157\n",
            "Epoch 8/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 1.1344 - acc: 0.6132 - val_loss: 1.1374 - val_acc: 0.6574\n",
            "Epoch 9/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 1.0675 - acc: 0.6243 - val_loss: 1.1614 - val_acc: 0.6528\n",
            "Epoch 10/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.9974 - acc: 0.6709 - val_loss: 1.3707 - val_acc: 0.6250\n",
            "Epoch 11/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.8999 - acc: 0.7105 - val_loss: 2.0742 - val_acc: 0.5278\n",
            "Epoch 12/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.9518 - acc: 0.6710 - val_loss: 1.1665 - val_acc: 0.6759\n",
            "Epoch 13/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.9795 - acc: 0.6750 - val_loss: 1.1793 - val_acc: 0.6019\n",
            "Epoch 14/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.8158 - acc: 0.6956 - val_loss: 1.7327 - val_acc: 0.5370\n",
            "Epoch 15/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.8110 - acc: 0.7224 - val_loss: 1.5251 - val_acc: 0.5972\n",
            "Epoch 16/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.7919 - acc: 0.7160 - val_loss: 1.1616 - val_acc: 0.6713\n",
            "Epoch 17/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.7178 - acc: 0.7369 - val_loss: 2.2880 - val_acc: 0.4769\n",
            "Epoch 18/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.6520 - acc: 0.7673 - val_loss: 1.0282 - val_acc: 0.6574\n",
            "Epoch 19/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.6204 - acc: 0.7648 - val_loss: 1.1793 - val_acc: 0.7037\n",
            "Epoch 20/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.7355 - acc: 0.7507 - val_loss: 1.0291 - val_acc: 0.7130\n",
            "Epoch 21/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.7190 - acc: 0.7481 - val_loss: 1.1987 - val_acc: 0.6528\n",
            "Epoch 22/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.6360 - acc: 0.7572 - val_loss: 1.4323 - val_acc: 0.6898\n",
            "Epoch 23/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.6516 - acc: 0.7718 - val_loss: 1.4076 - val_acc: 0.6204\n",
            "Epoch 24/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.6302 - acc: 0.7768 - val_loss: 1.5836 - val_acc: 0.6343\n",
            "Epoch 25/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.6243 - acc: 0.7463 - val_loss: 1.8698 - val_acc: 0.5370\n",
            "Epoch 26/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.6665 - acc: 0.7682 - val_loss: 1.9134 - val_acc: 0.5324\n",
            "Epoch 27/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.5359 - acc: 0.8001 - val_loss: 1.5192 - val_acc: 0.5648\n",
            "Epoch 28/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.5832 - acc: 0.7933 - val_loss: 2.2268 - val_acc: 0.4907\n",
            "Epoch 29/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.5690 - acc: 0.7780 - val_loss: 1.1343 - val_acc: 0.6852\n",
            "Epoch 30/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.5383 - acc: 0.8118 - val_loss: 1.2982 - val_acc: 0.6852\n",
            "Epoch 31/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.5327 - acc: 0.7926 - val_loss: 0.9580 - val_acc: 0.7130\n",
            "Epoch 32/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.4816 - acc: 0.8173 - val_loss: 1.3602 - val_acc: 0.6574\n",
            "Epoch 33/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.5472 - acc: 0.8069 - val_loss: 0.8092 - val_acc: 0.7500\n",
            "Epoch 34/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.4588 - acc: 0.8255 - val_loss: 1.0973 - val_acc: 0.6944\n",
            "Epoch 35/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.4245 - acc: 0.8379 - val_loss: 1.1651 - val_acc: 0.6944\n",
            "Epoch 36/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.5019 - acc: 0.8079 - val_loss: 1.0584 - val_acc: 0.7130\n",
            "Epoch 37/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.4789 - acc: 0.8187 - val_loss: 1.4209 - val_acc: 0.6667\n",
            "Epoch 38/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.3866 - acc: 0.8601 - val_loss: 1.3458 - val_acc: 0.6759\n",
            "Epoch 39/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.4593 - acc: 0.8300 - val_loss: 0.8437 - val_acc: 0.7361\n",
            "Epoch 40/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.4444 - acc: 0.8291 - val_loss: 0.9780 - val_acc: 0.7546\n",
            "Epoch 41/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.4537 - acc: 0.8366 - val_loss: 1.2704 - val_acc: 0.7037\n",
            "Epoch 42/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.4713 - acc: 0.8206 - val_loss: 0.9552 - val_acc: 0.7037\n",
            "Epoch 43/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.3768 - acc: 0.8480 - val_loss: 1.3159 - val_acc: 0.6435\n",
            "Epoch 44/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.4656 - acc: 0.8367 - val_loss: 1.3355 - val_acc: 0.6667\n",
            "Epoch 45/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.3748 - acc: 0.8577 - val_loss: 1.1127 - val_acc: 0.7083\n",
            "Epoch 46/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.4226 - acc: 0.8630 - val_loss: 1.1726 - val_acc: 0.6852\n",
            "Epoch 47/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.3588 - acc: 0.8769 - val_loss: 0.7875 - val_acc: 0.7685\n",
            "Epoch 48/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.3863 - acc: 0.8608 - val_loss: 1.0827 - val_acc: 0.6944\n",
            "Epoch 49/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.3721 - acc: 0.8563 - val_loss: 1.8929 - val_acc: 0.6157\n",
            "Epoch 50/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.3452 - acc: 0.8613 - val_loss: 1.5773 - val_acc: 0.6481\n",
            "Epoch 51/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.3637 - acc: 0.8709 - val_loss: 0.8302 - val_acc: 0.7361\n",
            "Epoch 52/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.3585 - acc: 0.8654 - val_loss: 1.2113 - val_acc: 0.6574\n",
            "Epoch 53/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.3805 - acc: 0.8661 - val_loss: 1.1230 - val_acc: 0.7176\n",
            "Epoch 54/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.3959 - acc: 0.8560 - val_loss: 1.0237 - val_acc: 0.7454\n",
            "Epoch 55/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.3811 - acc: 0.8678 - val_loss: 1.5306 - val_acc: 0.6852\n",
            "Epoch 56/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.3880 - acc: 0.8729 - val_loss: 1.2419 - val_acc: 0.6713\n",
            "Epoch 57/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.3437 - acc: 0.8722 - val_loss: 1.2126 - val_acc: 0.7037\n",
            "Epoch 58/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.3052 - acc: 0.8910 - val_loss: 2.2027 - val_acc: 0.5741\n",
            "Epoch 59/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.3334 - acc: 0.8827 - val_loss: 1.5319 - val_acc: 0.6528\n",
            "Epoch 60/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.3613 - acc: 0.8564 - val_loss: 0.9303 - val_acc: 0.7315\n",
            "Epoch 61/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.3493 - acc: 0.8580 - val_loss: 1.5219 - val_acc: 0.6250\n",
            "Epoch 62/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.3244 - acc: 0.8826 - val_loss: 1.0912 - val_acc: 0.7083\n",
            "Epoch 63/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.2913 - acc: 0.9021 - val_loss: 1.4489 - val_acc: 0.6574\n",
            "Epoch 64/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.3051 - acc: 0.8927 - val_loss: 0.9372 - val_acc: 0.7639\n",
            "Epoch 65/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.3674 - acc: 0.8671 - val_loss: 1.2172 - val_acc: 0.7130\n",
            "Epoch 66/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.3183 - acc: 0.8733 - val_loss: 1.3754 - val_acc: 0.7037\n",
            "Epoch 67/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.2937 - acc: 0.8831 - val_loss: 0.9705 - val_acc: 0.7824\n",
            "Epoch 68/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.2672 - acc: 0.9006 - val_loss: 1.0842 - val_acc: 0.7407\n",
            "Epoch 69/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.2612 - acc: 0.9080 - val_loss: 1.1386 - val_acc: 0.7176\n",
            "Epoch 70/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.3342 - acc: 0.8855 - val_loss: 0.8576 - val_acc: 0.7546\n",
            "Epoch 71/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.3734 - acc: 0.8640 - val_loss: 1.7932 - val_acc: 0.6389\n",
            "Epoch 72/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.3387 - acc: 0.8767 - val_loss: 2.1726 - val_acc: 0.6065\n",
            "Epoch 73/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.3113 - acc: 0.8843 - val_loss: 1.4557 - val_acc: 0.6759\n",
            "Epoch 74/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.4097 - acc: 0.8590 - val_loss: 1.5783 - val_acc: 0.6806\n",
            "Epoch 75/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.3671 - acc: 0.8695 - val_loss: 1.2814 - val_acc: 0.7176\n",
            "Epoch 76/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.3119 - acc: 0.8913 - val_loss: 1.5505 - val_acc: 0.6806\n",
            "Epoch 77/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.3224 - acc: 0.8815 - val_loss: 1.0365 - val_acc: 0.7639\n",
            "Epoch 78/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.3264 - acc: 0.8867 - val_loss: 1.0888 - val_acc: 0.7361\n",
            "Epoch 79/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.3464 - acc: 0.8650 - val_loss: 1.5419 - val_acc: 0.6620\n",
            "Epoch 80/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.2812 - acc: 0.8984 - val_loss: 1.4254 - val_acc: 0.6759\n",
            "Epoch 81/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.2799 - acc: 0.9045 - val_loss: 2.2065 - val_acc: 0.6204\n",
            "Epoch 82/100\n",
            "26/26 [==============================] - 51s 2s/step - loss: 0.3029 - acc: 0.8810 - val_loss: 1.5656 - val_acc: 0.6898\n",
            "Epoch 83/100\n",
            "26/26 [==============================] - 51s 2s/step - loss: 0.2609 - acc: 0.9080 - val_loss: 1.0272 - val_acc: 0.7963\n",
            "Epoch 84/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.2426 - acc: 0.8994 - val_loss: 1.0206 - val_acc: 0.7546\n",
            "Epoch 85/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.2970 - acc: 0.8973 - val_loss: 1.3546 - val_acc: 0.7037\n",
            "Epoch 86/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.2604 - acc: 0.9064 - val_loss: 1.1769 - val_acc: 0.7176\n",
            "Epoch 87/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.2747 - acc: 0.8972 - val_loss: 1.0993 - val_acc: 0.7407\n",
            "Epoch 88/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.2288 - acc: 0.9200 - val_loss: 1.1152 - val_acc: 0.7593\n",
            "Epoch 89/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.2129 - acc: 0.9149 - val_loss: 0.9969 - val_acc: 0.7685\n",
            "Epoch 90/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.1980 - acc: 0.9210 - val_loss: 1.2402 - val_acc: 0.7315\n",
            "Epoch 91/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.2744 - acc: 0.9105 - val_loss: 1.3558 - val_acc: 0.7500\n",
            "Epoch 92/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.2643 - acc: 0.8992 - val_loss: 1.2062 - val_acc: 0.7269\n",
            "Epoch 93/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.2305 - acc: 0.9241 - val_loss: 1.1349 - val_acc: 0.7500\n",
            "Epoch 94/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.2236 - acc: 0.9207 - val_loss: 1.2887 - val_acc: 0.6944\n",
            "Epoch 95/100\n",
            "26/26 [==============================] - 51s 2s/step - loss: 0.2222 - acc: 0.9191 - val_loss: 1.1663 - val_acc: 0.7546\n",
            "Epoch 96/100\n",
            "26/26 [==============================] - 51s 2s/step - loss: 0.2371 - acc: 0.9224 - val_loss: 1.1290 - val_acc: 0.7500\n",
            "Epoch 97/100\n",
            "26/26 [==============================] - 51s 2s/step - loss: 0.2118 - acc: 0.9248 - val_loss: 1.3887 - val_acc: 0.7269\n",
            "Epoch 98/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.2407 - acc: 0.9057 - val_loss: 3.4491 - val_acc: 0.5602\n",
            "Epoch 99/100\n",
            "26/26 [==============================] - 50s 2s/step - loss: 0.2531 - acc: 0.8910 - val_loss: 1.1863 - val_acc: 0.6898\n",
            "Epoch 100/100\n",
            "26/26 [==============================] - 51s 2s/step - loss: 0.2007 - acc: 0.9336 - val_loss: 1.3194 - val_acc: 0.7361\n",
            "[INFO] serializing network...\n",
            "[INFO] serializing label binarizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqCYg3DHma56",
        "colab_type": "text"
      },
      "source": [
        "用此單元格進行訓練，先定義三個參數dataset,modelz,labelbin，dataset為我們存取用來訓練的圖片路徑，modelz指定訓練完模型的輸出位置，labelbin為輸出標籤位置，並定義plot，畫圖儲存訓練結果，接著設定訓練次數epochs為100，IMAGE_DIMS中設定圖像為96X96像素並加上3個channel，載入dataset的所有圖像後隨機地移動，在imagePaths迴圈裡進行調整，對dataset的每個資料夾的圖像記標籤，讓程式訓練每個資料夾中對不同寶可夢的圖像辨識，最後儲存model 和label binarizer，並繪圖plot，可以從plot.png中看出圖形的訓練過程的精準度"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOiX-PXtgc8i",
        "colab_type": "text"
      },
      "source": [
        "https://www.pyimagesearch.com/2018/04/09/how-to-quickly-build-a-deep-learning-image-dataset/\n",
        "\n",
        "https://www.pyimagesearch.com/2018/04/16/keras-and-convolutional-neural-networks-cnns/\n",
        "\n",
        "https://qiita.com/LittleWat/items/6e56857e1f97c842b261\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8S9Lz2qS4vL",
        "colab_type": "code",
        "outputId": "29b99f57-7eba-4a20-f024-a41c8969d5bc",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 93
        }
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8c39b960-42f9-4e81-91eb-1a5f6cdd9fe6\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-8c39b960-42f9-4e81-91eb-1a5f6cdd9fe6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Charizard_counter.png to Charizard_counter.png\n",
            "User uploaded file \"Charizard_counter.png\" with length 91886 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qasN6Uw6unqb",
        "colab_type": "text"
      },
      "source": [
        "上傳測試圖片"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOR1hhDWuxAP",
        "colab_type": "code",
        "outputId": "fd410327-60f4-4d55-b995-fa4634cb1f6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        }
      },
      "source": [
        "# import the necessary packages\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.models import load_model\n",
        "import numpy as np\n",
        "import argparse\n",
        "import imutils\n",
        "import pickle\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "# construct the argument parse and parse the arguments\n",
        "'''ap = argparse.ArgumentParser()\n",
        "ap.add_argument(\"-m\", \"--model\", required=True,\n",
        "\thelp=\"path to trained model model\")\n",
        "ap.add_argument(\"-l\", \"--labelbin\", required=True,\n",
        "\thelp=\"path to label binarizer\")\n",
        "ap.add_argument(\"-i\", \"--image\", required=True,\n",
        "\thelp=\"path to input image\")\n",
        "args = vars(ap.parse_args())'''\n",
        "\n",
        "modelz='pokedex.model'\n",
        "labelbin='lb.pickle'\n",
        "image='Charizard_counter.png'\n",
        "\n",
        "# load the image\n",
        "#image = cv2.imread(args[\"image\"])\n",
        "image = cv2.imread(image)\n",
        "output = image.copy()\n",
        " \n",
        "# pre-process the image for classification\n",
        "image = cv2.resize(image, (96, 96))\n",
        "image = image.astype(\"float\") / 255.0\n",
        "image = img_to_array(image)\n",
        "image = np.expand_dims(image, axis=0)\n",
        "\n",
        "# load the trained convolutional neural network and the label\n",
        "# binarizer\n",
        "print(\"[INFO] loading network...\")\n",
        "#model = load_model(args[\"model\"])\n",
        "model = load_model(modelz)\n",
        "#lb = pickle.loads(open(args[\"labelbin\"], \"rb\").read())\n",
        "lb = pickle.loads(open(labelbin, \"rb\").read())\n",
        " \n",
        "# classify the input image\n",
        "print(\"[INFO] classifying image...\")\n",
        "proba = model.predict(image)[0]\n",
        "idx = np.argmax(proba)\n",
        "label = lb.classes_[idx]\n",
        "\n",
        "# we'll mark our prediction as \"correct\" of the input image filename\n",
        "# contains the predicted label text (obviously this makes the\n",
        "# assumption that you have named your testing image files this way)\n",
        "#filename = args[\"image\"][args[\"image\"].rfind(os.path.sep) + 1:]\n",
        "filename = image[image.rfind(os.path.sep) + 1:]\n",
        "correct = \"correct\" if filename.rfind(label) != -1 else \"incorrect\"\n",
        " \n",
        "# build the label and draw the label on the image\n",
        "label = \"{}: {:.2f}% ({})\".format(label, proba[idx] * 100, correct)\n",
        "#label = \"{}: {:.2f}% \".format(label, proba[idx] * 100)\n",
        "output = imutils.resize(output, width=400)\n",
        "cv2.putText(output, label, (10, 25),  cv2.FONT_HERSHEY_SIMPLEX,\n",
        "\t0.7, (0, 255, 0), 2)\n",
        " \n",
        "# show the output image\n",
        "print(\"[INFO] {}\".format(label))\n",
        "cv2.imshow(\"Output\", output)\n",
        "cv2.waitKey(0)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading network...\n",
            "[INFO] classifying image...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-ba984d830322>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# assumption that you have named your testing image files this way)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m#filename = args[\"image\"][args[\"image\"].rfind(os.path.sep) + 1:]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"correct\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"incorrect\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'rfind'"
          ]
        }
      ]
    }
  ]
}