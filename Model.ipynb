{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMEuIBZLXrYzkjFRD38AQcX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karta2552442a/jacky/blob/master/Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_WjNqn0j2AE"
      },
      "source": [
        "!mkdir test # 建立資料夾"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iK8l-z4EUFz",
        "outputId": "3b54e24b-1a4b-49da-e86b-a164bf93034b"
      },
      "source": [
        "# scikit-learn\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import Ridge, LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "# 測試檔案\n",
        "data_train = pd.read_csv('test/Training_set.csv', header=None).to_numpy()\n",
        "data_test = pd.read_csv('test/Validation_set.csv', header=None).to_numpy()\n",
        "\n",
        "train_array = data_train\n",
        "data_number = len(train_array[:, 0])\n",
        "feature_number = len(train_array[0, :])-1\n",
        "x_train = train_array[:, :feature_number]\n",
        "y_train = train_array[:, feature_number]\n",
        "\n",
        "test_array = data_test\n",
        "test_number = len(test_array[:, 0])\n",
        "x_test = test_array[:, :feature_number]\n",
        "y_test = test_array[:, feature_number]\n",
        "\n",
        "model = DecisionTreeRegressor()  # 選擇Model\n",
        "model.fit(x_train, y_train)  # 訓練\n",
        "y_predictions = model.predict(x_test)  # 預測\n",
        "\n",
        "print(\"prediction:\", y_predictions)\n",
        "print(\"true values:\", y_test)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "prediction: [0.595      0.875      0.64       0.71       0.97       0.82\n",
            " 0.58       0.84       0.76       0.81       0.78       0.79\n",
            " 0.62       0.72       0.71       0.75       0.875      0.58\n",
            " 0.86       0.77       0.43       0.75       0.79       0.78\n",
            " 0.91       0.945      0.76       0.76       0.58       0.5\n",
            " 0.69       0.81333333 0.69       0.57       0.46       0.95\n",
            " 0.86       0.79       0.52       0.82       0.685      0.81\n",
            " 0.64       0.76       0.71       0.7        0.875      0.78\n",
            " 0.54       0.63       0.74       0.61       0.76       0.76\n",
            " 0.74       0.96       0.9        0.34       0.92       0.92\n",
            " 0.92       0.86       0.48       0.94       0.92       0.94\n",
            " 0.79       0.77       0.8375     0.71       0.71       0.69666667\n",
            " 0.49       0.67       0.57       0.81       0.72       0.67\n",
            " 0.94       0.685      0.81333333 0.86       0.76       0.625\n",
            " 0.62       0.79       0.56       0.91       0.8        0.7\n",
            " 0.7        0.685      0.94       0.66       0.875      0.95\n",
            " 0.62       0.62       0.685      0.84      ]\n",
            "true values: [0.56 0.85 0.63 0.66 0.96 0.46 0.66 0.81 0.73 0.8  0.62 0.79 0.59 0.49\n",
            " 0.49 0.67 0.76 0.7  0.94 0.73 0.34 0.74 0.66 0.79 0.91 0.94 0.64 0.73\n",
            " 0.71 0.81 0.67 0.85 0.8  0.73 0.64 0.89 0.9  0.88 0.69 0.72 0.56 0.86\n",
            " 0.69 0.48 0.77 0.78 0.92 0.8  0.54 0.75 0.83 0.73 0.81 0.52 0.71 0.92\n",
            " 0.94 0.59 0.93 0.89 0.9  0.86 0.79 0.93 0.87 0.91 0.61 0.71 0.82 0.62\n",
            " 0.68 0.64 0.64 0.72 0.77 0.87 0.77 0.68 0.96 0.84 0.88 0.72 0.56 0.72\n",
            " 0.55 0.87 0.57 0.94 0.86 0.92 0.54 0.53 0.93 0.66 0.84 0.92 0.59 0.65\n",
            " 0.75 0.87]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfroRQ5S1wAL",
        "outputId": "3c4047bf-a0d7-4ac0-cea2-32f8d293379b"
      },
      "source": [
        "# Keras Sequential \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats as st\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import model_selection\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout,BatchNormalization\n",
        "from keras import optimizers\n",
        "from keras.optimizers import Adam\n",
        "from keras import backend as K\n",
        "\n",
        "# 測試資料\n",
        "data_train = pd.read_csv('test/Training_set.csv', header=None).to_numpy()\n",
        "data_test = pd.read_csv('test/Validation_set.csv', header=None).to_numpy()\n",
        "\n",
        "train_array = data_train\n",
        "data_number = len(train_array[:, 0])\n",
        "feature_number = len(train_array[0, :])-1\n",
        "x_train = train_array[:, :feature_number]\n",
        "y_train = train_array[:, feature_number]\n",
        "\n",
        "test_array = data_test\n",
        "test_number = len(test_array[:, 0])\n",
        "x_test = test_array[:, :feature_number]\n",
        "y_test = test_array[:, feature_number]\n",
        "\n",
        "# 誤差計算\n",
        "def rmse(y_pred,y_true):\n",
        "    return K.sqrt(K.mean(K.square(y_pred-y_true)))\n",
        "\n",
        "# 建立Sequential\n",
        "model=Sequential()\n",
        "model.add(Dense(256,input_dim=3,activation='relu'))\n",
        "model.add(Dense(256,activation='relu'))\n",
        "model.add(Dropout(0.08))\n",
        "model.add(Dense(1))\n",
        "\n",
        "# 編譯(損失函數,評估標準:RMSE 優化器:adam.adamax)\n",
        "#model.compile(loss=rmse,optimizer=\"adam\",metrics=[rmse])\n",
        "model.compile(loss=rmse,optimizer='Adamax',metrics=[rmse])\n",
        "\n",
        "# 訓練\n",
        "model.fit(x_train,y_train,epochs=200,batch_size=64)\n",
        "\n",
        "# 預測\n",
        "y_predictions=model.predict(x_test)\n",
        "\n",
        "print(\"prediction:\", y_predictions)\n",
        "print(\"true values:\", y_test)\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "5/5 [==============================] - 1s 4ms/step - loss: 19.0197 - rmse: 18.9213\n",
            "Epoch 2/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 15.2561 - rmse: 15.2510\n",
            "Epoch 3/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 10.5350 - rmse: 10.5212\n",
            "Epoch 4/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 7.6429 - rmse: 7.6796\n",
            "Epoch 5/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 6.8540 - rmse: 6.8423\n",
            "Epoch 6/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 6.3780 - rmse: 6.3703\n",
            "Epoch 7/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 5.8629 - rmse: 5.8583\n",
            "Epoch 8/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 5.8786 - rmse: 5.8911\n",
            "Epoch 9/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 6.2146 - rmse: 6.2181\n",
            "Epoch 10/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 5.3569 - rmse: 5.3506\n",
            "Epoch 11/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 5.8352 - rmse: 5.8268\n",
            "Epoch 12/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 5.3602 - rmse: 5.3529\n",
            "Epoch 13/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 4.5197 - rmse: 4.5060\n",
            "Epoch 14/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 4.6087 - rmse: 4.6203\n",
            "Epoch 15/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 4.3322 - rmse: 4.3455\n",
            "Epoch 16/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 4.0619 - rmse: 4.0714\n",
            "Epoch 17/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 4.0402 - rmse: 4.0255\n",
            "Epoch 18/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 3.5949 - rmse: 3.5822\n",
            "Epoch 19/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 3.2258 - rmse: 3.2351\n",
            "Epoch 20/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 3.1391 - rmse: 3.1295\n",
            "Epoch 21/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 3.1985 - rmse: 3.1914\n",
            "Epoch 22/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 2.9717 - rmse: 2.9729\n",
            "Epoch 23/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 2.8781 - rmse: 2.8713\n",
            "Epoch 24/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 3.0047 - rmse: 3.0025\n",
            "Epoch 25/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 2.6046 - rmse: 2.5961\n",
            "Epoch 26/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 2.5503 - rmse: 2.5541\n",
            "Epoch 27/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 2.5959 - rmse: 2.5970\n",
            "Epoch 28/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 2.2813 - rmse: 2.2860\n",
            "Epoch 29/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 2.3619 - rmse: 2.3612\n",
            "Epoch 30/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 2.2159 - rmse: 2.2207\n",
            "Epoch 31/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 2.2625 - rmse: 2.2650\n",
            "Epoch 32/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 1.8345 - rmse: 1.8293\n",
            "Epoch 33/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 2.0248 - rmse: 2.0261\n",
            "Epoch 34/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 1.8325 - rmse: 1.8335\n",
            "Epoch 35/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 1.9076 - rmse: 1.9026\n",
            "Epoch 36/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 1.6577 - rmse: 1.6602\n",
            "Epoch 37/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 1.9130 - rmse: 1.9033\n",
            "Epoch 38/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 1.8010 - rmse: 1.7921\n",
            "Epoch 39/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 1.0871 - rmse: 1.0872\n",
            "Epoch 40/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 1.0048 - rmse: 1.0022\n",
            "Epoch 41/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.8650 - rmse: 0.8668\n",
            "Epoch 42/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.8422 - rmse: 0.8442\n",
            "Epoch 43/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.7112 - rmse: 0.7089\n",
            "Epoch 44/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.8479 - rmse: 0.8523\n",
            "Epoch 45/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.7139 - rmse: 0.7136\n",
            "Epoch 46/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.5848 - rmse: 0.5823\n",
            "Epoch 47/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.7097 - rmse: 0.7061\n",
            "Epoch 48/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.4998 - rmse: 0.4995\n",
            "Epoch 49/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.4651 - rmse: 0.4643\n",
            "Epoch 50/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.4222 - rmse: 0.4221\n",
            "Epoch 51/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.2827 - rmse: 0.2826\n",
            "Epoch 52/200\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.2617 - rmse: 0.2619\n",
            "Epoch 53/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.2835 - rmse: 0.2820\n",
            "Epoch 54/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.2272 - rmse: 0.2260\n",
            "Epoch 55/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1581 - rmse: 0.1580\n",
            "Epoch 56/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1528 - rmse: 0.1531\n",
            "Epoch 57/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1430 - rmse: 0.1428\n",
            "Epoch 58/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1468 - rmse: 0.1469\n",
            "Epoch 59/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1586 - rmse: 0.1587\n",
            "Epoch 60/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1485 - rmse: 0.1481\n",
            "Epoch 61/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1299 - rmse: 0.1299\n",
            "Epoch 62/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1416 - rmse: 0.1424\n",
            "Epoch 63/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1569 - rmse: 0.1570\n",
            "Epoch 64/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1408 - rmse: 0.1413\n",
            "Epoch 65/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1455 - rmse: 0.1461\n",
            "Epoch 66/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1466 - rmse: 0.1466\n",
            "Epoch 67/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1353 - rmse: 0.1351\n",
            "Epoch 68/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1253 - rmse: 0.1256\n",
            "Epoch 69/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1339 - rmse: 0.1339\n",
            "Epoch 70/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1159 - rmse: 0.1159\n",
            "Epoch 71/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1290 - rmse: 0.1291\n",
            "Epoch 72/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1265 - rmse: 0.1260\n",
            "Epoch 73/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1299 - rmse: 0.1302\n",
            "Epoch 74/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1244 - rmse: 0.1243\n",
            "Epoch 75/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1330 - rmse: 0.1337\n",
            "Epoch 76/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1193 - rmse: 0.1196\n",
            "Epoch 77/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1189 - rmse: 0.1187\n",
            "Epoch 78/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1271 - rmse: 0.1266\n",
            "Epoch 79/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1305 - rmse: 0.1302\n",
            "Epoch 80/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1214 - rmse: 0.1211\n",
            "Epoch 81/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1240 - rmse: 0.1242\n",
            "Epoch 82/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1250 - rmse: 0.1250\n",
            "Epoch 83/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1201 - rmse: 0.1201\n",
            "Epoch 84/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1259 - rmse: 0.1264\n",
            "Epoch 85/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1191 - rmse: 0.1194\n",
            "Epoch 86/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1247 - rmse: 0.1247\n",
            "Epoch 87/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1267 - rmse: 0.1265\n",
            "Epoch 88/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1219 - rmse: 0.1222\n",
            "Epoch 89/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1228 - rmse: 0.1229\n",
            "Epoch 90/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1230 - rmse: 0.1232\n",
            "Epoch 91/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1392 - rmse: 0.1404\n",
            "Epoch 92/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1346 - rmse: 0.1346\n",
            "Epoch 93/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1258 - rmse: 0.1260\n",
            "Epoch 94/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1147 - rmse: 0.1150\n",
            "Epoch 95/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1075 - rmse: 0.1077\n",
            "Epoch 96/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1174 - rmse: 0.1170\n",
            "Epoch 97/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1311 - rmse: 0.1312\n",
            "Epoch 98/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1253 - rmse: 0.1249\n",
            "Epoch 99/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1228 - rmse: 0.1226\n",
            "Epoch 100/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1257 - rmse: 0.1259\n",
            "Epoch 101/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1241 - rmse: 0.1243\n",
            "Epoch 102/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1187 - rmse: 0.1189\n",
            "Epoch 103/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1214 - rmse: 0.1215\n",
            "Epoch 104/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1282 - rmse: 0.1284\n",
            "Epoch 105/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1331 - rmse: 0.1334\n",
            "Epoch 106/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1370 - rmse: 0.1370\n",
            "Epoch 107/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1283 - rmse: 0.1282\n",
            "Epoch 108/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1186 - rmse: 0.1185\n",
            "Epoch 109/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1202 - rmse: 0.1206\n",
            "Epoch 110/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1181 - rmse: 0.1182\n",
            "Epoch 111/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1301 - rmse: 0.1298\n",
            "Epoch 112/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1271 - rmse: 0.1270\n",
            "Epoch 113/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1370 - rmse: 0.1368\n",
            "Epoch 114/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1275 - rmse: 0.1274\n",
            "Epoch 115/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1331 - rmse: 0.1325\n",
            "Epoch 116/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1270 - rmse: 0.1267\n",
            "Epoch 117/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1109 - rmse: 0.1109\n",
            "Epoch 118/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1237 - rmse: 0.1234\n",
            "Epoch 119/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1367 - rmse: 0.1363\n",
            "Epoch 120/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1330 - rmse: 0.1330\n",
            "Epoch 121/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1334 - rmse: 0.1335\n",
            "Epoch 122/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1329 - rmse: 0.1325\n",
            "Epoch 123/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1287 - rmse: 0.1288\n",
            "Epoch 124/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1297 - rmse: 0.1296\n",
            "Epoch 125/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1194 - rmse: 0.1196\n",
            "Epoch 126/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1382 - rmse: 0.1385\n",
            "Epoch 127/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1399 - rmse: 0.1398\n",
            "Epoch 128/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1416 - rmse: 0.1414\n",
            "Epoch 129/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1254 - rmse: 0.1252\n",
            "Epoch 130/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1277 - rmse: 0.1279\n",
            "Epoch 131/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1182 - rmse: 0.1176\n",
            "Epoch 132/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1186 - rmse: 0.1184\n",
            "Epoch 133/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1081 - rmse: 0.1079\n",
            "Epoch 134/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1173 - rmse: 0.1173\n",
            "Epoch 135/200\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.1186 - rmse: 0.1187\n",
            "Epoch 136/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1209 - rmse: 0.1207\n",
            "Epoch 137/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1183 - rmse: 0.1182\n",
            "Epoch 138/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1210 - rmse: 0.1210\n",
            "Epoch 139/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1222 - rmse: 0.1223\n",
            "Epoch 140/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1151 - rmse: 0.1149\n",
            "Epoch 141/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1141 - rmse: 0.1140\n",
            "Epoch 142/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1134 - rmse: 0.1133\n",
            "Epoch 143/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1115 - rmse: 0.1118\n",
            "Epoch 144/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1201 - rmse: 0.1201\n",
            "Epoch 145/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1372 - rmse: 0.1370\n",
            "Epoch 146/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1247 - rmse: 0.1246\n",
            "Epoch 147/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1322 - rmse: 0.1318\n",
            "Epoch 148/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1246 - rmse: 0.1245\n",
            "Epoch 149/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1264 - rmse: 0.1262\n",
            "Epoch 150/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1084 - rmse: 0.1086\n",
            "Epoch 151/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1241 - rmse: 0.1234\n",
            "Epoch 152/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1179 - rmse: 0.1180\n",
            "Epoch 153/200\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.1200 - rmse: 0.1199\n",
            "Epoch 154/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1219 - rmse: 0.1217\n",
            "Epoch 155/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1251 - rmse: 0.1247\n",
            "Epoch 156/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1268 - rmse: 0.1267\n",
            "Epoch 157/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1106 - rmse: 0.1109\n",
            "Epoch 158/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1235 - rmse: 0.1230\n",
            "Epoch 159/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1182 - rmse: 0.1178\n",
            "Epoch 160/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1121 - rmse: 0.1119\n",
            "Epoch 161/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1188 - rmse: 0.1187\n",
            "Epoch 162/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1137 - rmse: 0.1137\n",
            "Epoch 163/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1199 - rmse: 0.1197\n",
            "Epoch 164/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1138 - rmse: 0.1136\n",
            "Epoch 165/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1175 - rmse: 0.1179\n",
            "Epoch 166/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1272 - rmse: 0.1273\n",
            "Epoch 167/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1194 - rmse: 0.1192\n",
            "Epoch 168/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1353 - rmse: 0.1353\n",
            "Epoch 169/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1260 - rmse: 0.1264\n",
            "Epoch 170/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1248 - rmse: 0.1252\n",
            "Epoch 171/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1188 - rmse: 0.1188\n",
            "Epoch 172/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1203 - rmse: 0.1202\n",
            "Epoch 173/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1199 - rmse: 0.1199\n",
            "Epoch 174/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1088 - rmse: 0.1092\n",
            "Epoch 175/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1258 - rmse: 0.1258\n",
            "Epoch 176/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1256 - rmse: 0.1261\n",
            "Epoch 177/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1421 - rmse: 0.1415\n",
            "Epoch 178/200\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.1369 - rmse: 0.1366\n",
            "Epoch 179/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1305 - rmse: 0.1301\n",
            "Epoch 180/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1281 - rmse: 0.1276\n",
            "Epoch 181/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1257 - rmse: 0.1261\n",
            "Epoch 182/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1243 - rmse: 0.1249\n",
            "Epoch 183/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1320 - rmse: 0.1321\n",
            "Epoch 184/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1057 - rmse: 0.1059\n",
            "Epoch 185/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1167 - rmse: 0.1166\n",
            "Epoch 186/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1181 - rmse: 0.1184\n",
            "Epoch 187/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1159 - rmse: 0.1160\n",
            "Epoch 188/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1160 - rmse: 0.1160\n",
            "Epoch 189/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1108 - rmse: 0.1111\n",
            "Epoch 190/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1239 - rmse: 0.1241\n",
            "Epoch 191/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1142 - rmse: 0.1145\n",
            "Epoch 192/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1120 - rmse: 0.1118\n",
            "Epoch 193/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1133 - rmse: 0.1132\n",
            "Epoch 194/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1179 - rmse: 0.1173\n",
            "Epoch 195/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1175 - rmse: 0.1175\n",
            "Epoch 196/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1157 - rmse: 0.1157\n",
            "Epoch 197/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1190 - rmse: 0.1187\n",
            "Epoch 198/200\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.1213 - rmse: 0.1211\n",
            "Epoch 199/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1231 - rmse: 0.1231\n",
            "Epoch 200/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1116 - rmse: 0.1119\n",
            "prediction: [[0.6471277 ]\n",
            " [0.7515295 ]\n",
            " [0.6784764 ]\n",
            " [0.66268116]\n",
            " [0.8220157 ]\n",
            " [0.76325476]\n",
            " [0.6413471 ]\n",
            " [0.7334542 ]\n",
            " [0.75980794]\n",
            " [0.8009358 ]\n",
            " [0.7530488 ]\n",
            " [0.7137203 ]\n",
            " [0.63282406]\n",
            " [0.6868539 ]\n",
            " [0.64612323]\n",
            " [0.70186085]\n",
            " [0.71464473]\n",
            " [0.67789257]\n",
            " [0.8032067 ]\n",
            " [0.6814896 ]\n",
            " [0.5881757 ]\n",
            " [0.66848224]\n",
            " [0.7101392 ]\n",
            " [0.75072616]\n",
            " [0.80144256]\n",
            " [0.82172114]\n",
            " [0.63078976]\n",
            " [0.6604635 ]\n",
            " [0.669709  ]\n",
            " [0.70612216]\n",
            " [0.7432513 ]\n",
            " [0.735218  ]\n",
            " [0.7452597 ]\n",
            " [0.674215  ]\n",
            " [0.65193087]\n",
            " [0.77580494]\n",
            " [0.82755566]\n",
            " [0.7394793 ]\n",
            " [0.64084727]\n",
            " [0.73722625]\n",
            " [0.7123919 ]\n",
            " [0.84409827]\n",
            " [0.6453643 ]\n",
            " [0.6130675 ]\n",
            " [0.6575883 ]\n",
            " [0.6992939 ]\n",
            " [0.7392344 ]\n",
            " [0.65187246]\n",
            " [0.6142599 ]\n",
            " [0.71134394]\n",
            " [0.7414874 ]\n",
            " [0.6699541 ]\n",
            " [0.6644176 ]\n",
            " [0.6826312 ]\n",
            " [0.6456085 ]\n",
            " [0.80345106]\n",
            " [0.7605415 ]\n",
            " [0.6039906 ]\n",
            " [0.7786169 ]\n",
            " [0.7680858 ]\n",
            " [0.82561034]\n",
            " [0.7703385 ]\n",
            " [0.72908276]\n",
            " [0.8074678 ]\n",
            " [0.7652647 ]\n",
            " [0.7969368 ]\n",
            " [0.6465966 ]\n",
            " [0.71502334]\n",
            " [0.73120105]\n",
            " [0.61881727]\n",
            " [0.6493807 ]\n",
            " [0.6949119 ]\n",
            " [0.6349113 ]\n",
            " [0.70587736]\n",
            " [0.6400667 ]\n",
            " [0.78640574]\n",
            " [0.7028216 ]\n",
            " [0.643873  ]\n",
            " [0.795173  ]\n",
            " [0.7083748 ]\n",
            " [0.73521763]\n",
            " [0.7703385 ]\n",
            " [0.56141573]\n",
            " [0.66417307]\n",
            " [0.6639286 ]\n",
            " [0.7020852 ]\n",
            " [0.6088943 ]\n",
            " [0.74173236]\n",
            " [0.75579077]\n",
            " [0.7966004 ]\n",
            " [0.65388644]\n",
            " [0.71640885]\n",
            " [0.7705832 ]\n",
            " [0.6724513 ]\n",
            " [0.7146449 ]\n",
            " [0.8114843 ]\n",
            " [0.63507754]\n",
            " [0.6639286 ]\n",
            " [0.718417  ]\n",
            " [0.7334542 ]]\n",
            "true values: [0.56 0.85 0.63 0.66 0.96 0.46 0.66 0.81 0.73 0.8  0.62 0.79 0.59 0.49\n",
            " 0.49 0.67 0.76 0.7  0.94 0.73 0.34 0.74 0.66 0.79 0.91 0.94 0.64 0.73\n",
            " 0.71 0.81 0.67 0.85 0.8  0.73 0.64 0.89 0.9  0.88 0.69 0.72 0.56 0.86\n",
            " 0.69 0.48 0.77 0.78 0.92 0.8  0.54 0.75 0.83 0.73 0.81 0.52 0.71 0.92\n",
            " 0.94 0.59 0.93 0.89 0.9  0.86 0.79 0.93 0.87 0.91 0.61 0.71 0.82 0.62\n",
            " 0.68 0.64 0.64 0.72 0.77 0.87 0.77 0.68 0.96 0.84 0.88 0.72 0.56 0.72\n",
            " 0.55 0.87 0.57 0.94 0.86 0.92 0.54 0.53 0.93 0.66 0.84 0.92 0.59 0.65\n",
            " 0.75 0.87]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MC8G0BbhEc1n",
        "outputId": "fe47340f-02a4-4ed1-d098-4bbdaa0b7988"
      },
      "source": [
        "# LSTM(暫)\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "import numpy as np\n",
        "\n",
        "data_dim = 16\n",
        "timesteps = 8\n",
        "num_classes = 10\n",
        "\n",
        "# 期望輸入資料尺寸: (batch_size, timesteps, data_dim)\n",
        "model = Sequential()\n",
        "model.add(LSTM(32, return_sequences=True,\n",
        "input_shape=(timesteps, data_dim)))# 返回維度為 32 的向量序列\n",
        "model.add(LSTM(32, return_sequences=True))# 返回維度為 32 的向量序列\n",
        "model.add(LSTM(32))# 返回維度為 32 的單個向量\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
        "\n",
        "# 生成虛擬訓練資料\n",
        "x_train = np.random.random((1000, timesteps, data_dim))\n",
        "y_train = np.random.random((1000, num_classes))\n",
        "\n",
        "# 生成虛擬驗證資料\n",
        "x_val = np.random.random((100, timesteps, data_dim))\n",
        "y_val = np.random.random((100, num_classes))\n",
        "\n",
        "model.fit(x_train, y_train,batch_size=64, epochs=5,validation_data=(x_val, y_val))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "16/16 [==============================] - 8s 106ms/step - loss: 11.6271 - accuracy: 0.1088 - val_loss: 12.3880 - val_accuracy: 0.1100\n",
            "Epoch 2/5\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 12.6935 - accuracy: 0.0893 - val_loss: 12.9610 - val_accuracy: 0.1100\n",
            "Epoch 3/5\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 13.3815 - accuracy: 0.0850 - val_loss: 13.1995 - val_accuracy: 0.1100\n",
            "Epoch 4/5\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 13.3923 - accuracy: 0.0927 - val_loss: 13.3916 - val_accuracy: 0.1100\n",
            "Epoch 5/5\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 13.6359 - accuracy: 0.0984 - val_loss: 13.6221 - val_accuracy: 0.1100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f15e3290290>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    }
  ]
}